{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2681240-a25c-4533-910f-65be2e108afe",
   "metadata": {},
   "source": [
    "# Salmon Quantification Data Processing and Filtering Workflow\n",
    "## Author: Felipe Villena, PhD\n",
    "###  Date: 25-07-2024\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This project provides functionality analogous to the tximport package in R, \n",
    "but implemented in Python. It efficiently imports and processes data from \n",
    "Salmon Quantification files, consolidating multiple files into a single table.\n",
    "\n",
    "The script can extract various types of data, including **Transcripts Per Million (TPM)**\n",
    "and **Read Counts (NumReads)** , among others.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "1. Parallel processing of multiple Salmon quantification files\n",
    "2. Flexible data extraction (e.g., TPM, NumReads, or other columns from Salmon output)\n",
    "3. Consolidation of multiple Salmon output files into a single comprehensive table\n",
    "4. Efficient batch processing and merging of large datasets\n",
    "5. Custom filtering options for both rows (genes) and columns (samples)\n",
    "\n",
    "**Usage:**\n",
    "\n",
    "Modify the 'columns_to_extract' variable to specify which data type(s) to extract \n",
    "(e.g., ['TPM'], ['NumReads'], or ['TPM', 'NumReads'] for both).\n",
    "Adjust input and output file paths as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d04a99-4970-44c0-a95a-2d6b20d666f2",
   "metadata": {},
   "source": [
    "# 1 - Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80971840-977a-4939-b1f9-d65ee0b65921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74ea987-7c48-4b9f-9407-0e1d296b0a75",
   "metadata": {},
   "source": [
    "# 2 - Function Definitions\n",
    "\n",
    "### process_file(file, columns_to_extract)\n",
    "This function processes a single Salmon quantification file. It extracts specified columns (e.g., TPM, NumReads) and renames them with the sample identifier.\n",
    "\n",
    "### process_batch(batch_files, output_file, batch_num, columns_to_extract)\n",
    "This function processes a batch of Salmon quantification files. It's designed to work with the multiprocessing pool for parallel processing.\n",
    "\n",
    "### merge_batch_files(temp_files, output_file)\n",
    "This function merges the processed batch files into a single comprehensive table. It handles the consolidation of data from multiple Salmon output files.\n",
    "\n",
    "### main(input_folder, output_file, columns_to_extract, batch_size=100)\n",
    "\n",
    "The main function orchestrates the entire workflow. It's structured to:\n",
    "\n",
    "1. Locate and list all Salmon quantification files\n",
    "2. Set up parallel processing using Python's multiprocessing\n",
    "3. Process files in batches\n",
    "4. Merge results into a single output file\n",
    "\n",
    "This structure allows for efficient processing of large datasets by:\n",
    "- Utilizing parallel processing to speed up file reading and initial processing\n",
    "- Processing files in batches to manage memory usage\n",
    "- Consolidating results incrementally to handle large numbers of samples\n",
    "\n",
    "### filter_dataset(input_file, output_file, row_condition=None, column_condition=None)\n",
    "This function applies filtering to the consolidated dataset. It can filter both rows (e.g., genes) and columns (e.g., samples) based on specified conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e83063-a335-42a2-857b-19e814595953",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_config(config_file):\n",
    "    \"\"\"Load configuration from YAML file.\"\"\"\n",
    "    with open(config_file, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "def process_file(file, columns_to_extract, sample_id_pattern):\n",
    "    \"\"\"\n",
    "    Process a single Salmon quantification file.\n",
    "    \n",
    "    Args:\n",
    "    file (str): Path to the Salmon quantification file.\n",
    "    columns_to_extract (list): List of column names to extract from the file.\n",
    "    sample_id_pattern (str): Regular expression pattern to extract sample ID from filename.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Processed data from the file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sample_name_match = re.search(sample_id_pattern, file)\n",
    "        sample_name = sample_name_match.group('sample_id') if sample_name_match else \"unknown\"\n",
    "        \n",
    "        usecols = ['Name'] + columns_to_extract\n",
    "        sample_data = pd.read_csv(file, sep='\\t', usecols=usecols)\n",
    "        \n",
    "        sample_data = sample_data.rename(columns={col: f'{sample_name}' for col in columns_to_extract})\n",
    "        sample_data = sample_data.set_index('Name')\n",
    "        \n",
    "        return sample_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_batch(batch_files, output_file, batch_num, columns_to_extract, sample_id_pattern):\n",
    "    \"\"\"\n",
    "    Process a batch of Salmon quantification files.\n",
    "    \n",
    "    Args:\n",
    "    batch_files (list): List of file paths to process.\n",
    "    output_file (str): Base name for the output file.\n",
    "    batch_num (int): Batch number for identification.\n",
    "    columns_to_extract (list): List of column names to extract from each file.\n",
    "    sample_id_pattern (str): Regular expression pattern to extract sample ID from filename.\n",
    "    \n",
    "    Returns:\n",
    "    str: Path to the temporary batch file.\n",
    "    \"\"\"\n",
    "    print(f\"Processing batch {batch_num} with {len(batch_files)} files\")\n",
    "    batch_data = [process_file(file, columns_to_extract, sample_id_pattern) for file in batch_files]\n",
    "    batch_data = [data for data in batch_data if data is not None]\n",
    "    \n",
    "    if not batch_data:\n",
    "        print(f\"Warning: Batch {batch_num} has no valid data.\")\n",
    "        return None\n",
    "    \n",
    "    combined_data = pd.concat(batch_data, axis=1)\n",
    "    \n",
    "    temp_file = f\"{output_file}.batch{batch_num}\"\n",
    "    combined_data.to_csv(temp_file, sep='\\t')\n",
    "    \n",
    "    print(f\"Batch {batch_num} processed successfully. Shape: {combined_data.shape}\")\n",
    "    return temp_file\n",
    "\n",
    "def merge_batch_files(temp_files, output_file):\n",
    "    \"\"\"\n",
    "    Merge processed batch files into a single output file.\n",
    "    \n",
    "    Args:\n",
    "    temp_files (list): List of temporary batch file paths.\n",
    "    output_file (str): Path for the final output file.\n",
    "    \"\"\"\n",
    "    print(f\"Merging {len(temp_files)} batch files...\")\n",
    "    all_data = []\n",
    "    for i, temp_file in enumerate(tqdm(temp_files, desc=\"Reading batch files\")):\n",
    "        try:\n",
    "            if not os.path.exists(temp_file):\n",
    "                print(f\"Warning: Temp file {temp_file} does not exist.\")\n",
    "                continue\n",
    "            \n",
    "            batch_data = pd.read_csv(temp_file, sep='\\t', index_col='Name')\n",
    "            print(f\"Batch {i} shape: {batch_data.shape}\")\n",
    "            \n",
    "            if batch_data.empty:\n",
    "                print(f\"Warning: Batch {i} is empty.\")\n",
    "                continue\n",
    "            \n",
    "            all_data.append(batch_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {temp_file}: {str(e)}\")\n",
    "        finally:\n",
    "            if os.path.exists(temp_file):\n",
    "                os.remove(temp_file)\n",
    "    \n",
    "    if not all_data:\n",
    "        raise ValueError(\"No data was successfully read from temporary files.\")\n",
    "    \n",
    "    print(\"Concatenating all data...\")\n",
    "    final_data = pd.concat(all_data, axis=1)\n",
    "    print(f\"Final data shape: {final_data.shape}\")\n",
    "    \n",
    "    print(\"Writing final output...\")\n",
    "    final_data.to_csv(output_file, sep='\\t')\n",
    "\n",
    "def main(input_folder, output_file, columns_to_extract, batch_size, sample_id_pattern):\n",
    "    \"\"\"\n",
    "    Main function to process Salmon quantification files.\n",
    "    \n",
    "    Args:\n",
    "    input_folder (str): Path to the folder containing Salmon quantification files.\n",
    "    output_file (str): Path for the final output file.\n",
    "    columns_to_extract (list): List of column names to extract from each file.\n",
    "    batch_size (int): Number of files to process in each batch.\n",
    "    sample_id_pattern (str): Regular expression pattern to extract sample ID from filename.\n",
    "    \"\"\"\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    print(f\"Looking for files in: {os.path.abspath(input_folder)}\")\n",
    "\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Error: The folder '{input_folder}' does not exist in the current directory.\")\n",
    "        print(\"Contents of current directory:\")\n",
    "        print(os.listdir())\n",
    "        return\n",
    "\n",
    "    files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith('.sf')]\n",
    "    print(f\"Total .sf files found: {len(files)}\")\n",
    "\n",
    "    if not files:\n",
    "        print(\"Error: No files found to process. Please check the file extension and directory.\")\n",
    "        return\n",
    "\n",
    "    num_cores = mp.cpu_count() - 1\n",
    "    batches = [files[i:i+batch_size] for i in range(0, len(files), batch_size)]\n",
    "\n",
    "    pool = mp.Pool(num_cores)\n",
    "\n",
    "    results = []\n",
    "    with tqdm(total=len(batches), desc=\"Processing batches\") as pbar:\n",
    "        for result in pool.starmap(process_batch, [(batch, output_file, i, columns_to_extract, sample_id_pattern) for i, batch in enumerate(batches)]):\n",
    "            if result is not None:\n",
    "                results.append(result)\n",
    "            else:\n",
    "                print(f\"Batch {len(results)} failed to process\")\n",
    "            pbar.update()\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    if not results:\n",
    "        print(\"Error: No batches were successfully processed. Check the logs above for details.\")\n",
    "    else:\n",
    "        print(f\"Successfully processed {len(results)} batches out of {len(batches)}\")\n",
    "        merge_batch_files(results, output_file)\n",
    "\n",
    "    print(f\"Processing complete. Output written to {output_file}\")\n",
    "\n",
    "def filter_dataset(input_file, output_file, row_condition=None, column_condition=None):\n",
    "    \"\"\"\n",
    "    Filter the processed dataset based on row and column conditions.\n",
    "    \n",
    "    Args:\n",
    "    input_file (str): Path to the input file to filter.\n",
    "    output_file (str): Path for the filtered output file.\n",
    "    row_condition (list or callable): Condition to filter rows.\n",
    "    column_condition (list or callable): Condition to filter columns.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {input_file}\")\n",
    "    df = pd.read_csv(input_file, sep='\\t', index_col='Name')\n",
    "    \n",
    "    original_shape = df.shape\n",
    "    print(f\"Original dataset shape: {original_shape}\")\n",
    "\n",
    "    if row_condition is not None:\n",
    "        if callable(row_condition):\n",
    "            df = df.loc[df.index.map(row_condition)]\n",
    "        else:\n",
    "            df = df.loc[df.index.isin(row_condition)]\n",
    "    \n",
    "    if column_condition is not None:\n",
    "        if callable(column_condition):\n",
    "            df = df.loc[:, df.columns.map(column_condition)]\n",
    "        else:\n",
    "            df = df.loc[:, df.columns.isin(column_condition)]\n",
    "    \n",
    "    print(f\"Filtered dataset shape: {df.shape}\")\n",
    "    \n",
    "    df.to_csv(output_file, sep='\\t')\n",
    "    print(f\"Filtered data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bec3ed5-1c3b-4c70-8c43-14190833e744",
   "metadata": {},
   "source": [
    "# 3 - Main Execution\n",
    "\n",
    "## Filtering the Consolidated Dataset\n",
    "\n",
    "After consolidating the Salmon quantification data, we often need to filter the dataset for specific analyses. In RNA-seq studies, it's common to filter both rows (genes) and columns (samples). Let's break down our filtering process:\n",
    "\n",
    "### Row Filtering (Genes)\n",
    "We filter rows to focus on genes of interest, often based on prior analyses or biological significance.\n",
    "\n",
    "In this example, we're using results from a meta-analysis of RNA-seq data in Parkinson's Disease:\n",
    "\n",
    "- We load a file 'Meta-analysis-RNAseq-PD-v2.csv' which contains genes ranked by their importance in PD.\n",
    "- We select the top 151 genes based on 'Feature_Importance_1'.\n",
    "- These genes likely represent those most relevant to PD based on the meta-analysis.\n",
    "\n",
    "### Column Filtering (Samples)\n",
    "We filter columns to select specific samples for our analysis, often based on clinical or quality control criteria.\n",
    "\n",
    "In this example:\n",
    "\n",
    "- We load a file 'PatientsSelected.tsv' which contains information about patients/samples.\n",
    "- We use the 'HudAlphaID' column to select specific samples.\n",
    "- This could represent a subset of patients meeting certain clinical criteria, quality thresholds, or belonging to a specific study group.\n",
    "\n",
    "### Applying the Filters\n",
    "\n",
    "The `filter_dataset` function applies these filters to our consolidated data:\n",
    "\n",
    "- Row condition (Feat_1): Keeps only the top 151 genes from our meta-analysis.\n",
    "- Column condition (Patients_filter): Keeps only the samples specified in our patient selection file.\n",
    "\n",
    "This filtered dataset (PPMI_PD_Counts_Feature1.tsv) will contain expression data for our genes of interest across the selected patient samples, ready for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14ec398f-a820-4e0a-8920-b505b12828bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/jovyan/mounted_data\n",
      "Looking for files in: /home/jovyan/mounted_data/quant_salmon_genes\n",
      "Total .sf files found: 4756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 4 with 100 filesProcessing batch 2 with 100 filesProcessing batch 0 with 100 filesProcessing batch 6 with 100 filesProcessing batch 12 with 100 files\n",
      "\n",
      "Processing batch 8 with 100 files\n",
      "\n",
      "Processing batch 10 with 100 files\n",
      "\n",
      "\n",
      "Batch 6 processed successfully. Shape: (58294, 100)\n",
      "Batch 8 processed successfully. Shape: (58294, 100)\n",
      "Batch 0 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 7 with 100 files\n",
      "Batch 12 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 9 with 100 files\n",
      "Processing batch 1 with 100 files\n",
      "Processing batch 13 with 100 files\n",
      "Batch 10 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 11 with 100 files\n",
      "Batch 4 processed successfully. Shape: (58294, 100)\n",
      "Batch 2 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 5 with 100 files\n",
      "Processing batch 3 with 100 files\n",
      "Batch 7 processed successfully. Shape: (58294, 100)\n",
      "Batch 13 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 14 with 100 files\n",
      "Processing batch 16 with 100 files\n",
      "Batch 1 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 18 with 100 files\n",
      "Batch 9 processed successfully. Shape: (58294, 100)\n",
      "Batch 5 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 20 with 100 files\n",
      "Processing batch 22 with 100 files\n",
      "Batch 11 processed successfully. Shape: (58294, 100)\n",
      "Batch 3 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 24 with 100 files\n",
      "Processing batch 26 with 100 files\n",
      "Batch 14 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 15 with 100 files\n",
      "Batch 18 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 19 with 100 files\n",
      "Batch 16 processed successfully. Shape: (58294, 100)\n",
      "Batch 22 processed successfully. Shape: (58294, 100)\n",
      "Batch 20 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 23 with 100 files\n",
      "Processing batch 17 with 100 files\n",
      "Processing batch 21 with 100 files\n",
      "Batch 24 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 25 with 100 files\n",
      "Batch 26 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 27 with 100 files\n",
      "Batch 15 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 28 with 100 files\n",
      "Batch 17 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 30 with 100 files\n",
      "Batch 23 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 32 with 100 files\n",
      "Batch 21 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 34 with 100 files\n",
      "Batch 19 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 36 with 100 files\n",
      "Batch 25 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 38 with 100 files\n",
      "Batch 27 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 40 with 100 files\n",
      "Batch 30 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 31 with 100 files\n",
      "Batch 34 processed successfully. Shape: (58294, 100)\n",
      "Batch 28 processed successfully. Shape: (58294, 100)\n",
      "Batch 32 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 35 with 100 files\n",
      "Processing batch 29 with 100 files\n",
      "Processing batch 33 with 100 files\n",
      "Batch 36 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 37 with 100 files\n",
      "Batch 38 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 39 with 100 files\n",
      "Batch 40 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 41 with 100 files\n",
      "Batch 31 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 42 with 100 files\n",
      "Batch 35 processed successfully. Shape: (58294, 100)\n",
      "Batch 29 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 44 with 100 files\n",
      "Batch 33 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 46 with 100 files\n",
      "Batch 37 processed successfully. Shape: (58294, 100)\n",
      "Batch 39 processed successfully. Shape: (58294, 100)\n",
      "Batch 41 processed successfully. Shape: (58294, 100)\n",
      "Batch 42 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 43 with 100 files\n",
      "Batch 44 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 45 with 100 files\n",
      "Batch 46 processed successfully. Shape: (58294, 100)\n",
      "Processing batch 47 with 56 files\n",
      "Batch 47 processed successfully. Shape: (58294, 56)\n",
      "Batch 43 processed successfully. Shape: (58294, 100)\n",
      "Batch 45 processed successfully. Shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 48/48 [00:53<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 48 batches out of 48\n",
      "Merging 48 batch files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:   2%|▏         | 1/48 [00:00<00:12,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:   4%|▍         | 2/48 [00:00<00:12,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:   6%|▋         | 3/48 [00:00<00:11,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:   8%|▊         | 4/48 [00:01<00:11,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  10%|█         | 5/48 [00:01<00:11,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  12%|█▎        | 6/48 [00:01<00:11,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  15%|█▍        | 7/48 [00:01<00:10,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  17%|█▋        | 8/48 [00:02<00:10,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  19%|█▉        | 9/48 [00:02<00:10,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  21%|██        | 10/48 [00:02<00:10,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  23%|██▎       | 11/48 [00:02<00:10,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  25%|██▌       | 12/48 [00:03<00:09,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  27%|██▋       | 13/48 [00:03<00:09,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  29%|██▉       | 14/48 [00:03<00:09,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  31%|███▏      | 15/48 [00:04<00:08,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 14 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  33%|███▎      | 16/48 [00:04<00:08,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 15 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  35%|███▌      | 17/48 [00:04<00:08,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 16 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  38%|███▊      | 18/48 [00:04<00:07,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 17 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  40%|███▉      | 19/48 [00:05<00:07,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 18 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  42%|████▏     | 20/48 [00:05<00:07,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 19 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  44%|████▍     | 21/48 [00:05<00:07,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  46%|████▌     | 22/48 [00:05<00:06,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 21 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  48%|████▊     | 23/48 [00:06<00:06,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 22 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  50%|█████     | 24/48 [00:06<00:06,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 23 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  52%|█████▏    | 25/48 [00:06<00:06,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 24 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  54%|█████▍    | 26/48 [00:07<00:06,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 25 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  56%|█████▋    | 27/48 [00:07<00:05,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 26 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  58%|█████▊    | 28/48 [00:07<00:05,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 27 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  60%|██████    | 29/48 [00:07<00:05,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 28 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  62%|██████▎   | 30/48 [00:08<00:04,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 29 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  65%|██████▍   | 31/48 [00:08<00:04,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 30 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  67%|██████▋   | 32/48 [00:08<00:04,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 31 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  69%|██████▉   | 33/48 [00:08<00:03,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  71%|███████   | 34/48 [00:09<00:03,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 33 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  73%|███████▎  | 35/48 [00:09<00:03,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 34 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  75%|███████▌  | 36/48 [00:09<00:03,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 35 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  77%|███████▋  | 37/48 [00:09<00:02,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 36 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  79%|███████▉  | 38/48 [00:10<00:02,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 37 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  81%|████████▏ | 39/48 [00:10<00:02,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 38 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  83%|████████▎ | 40/48 [00:10<00:02,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 39 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  85%|████████▌ | 41/48 [00:11<00:01,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 40 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  88%|████████▊ | 42/48 [00:11<00:01,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 41 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  90%|████████▉ | 43/48 [00:11<00:01,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 42 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  92%|█████████▏| 44/48 [00:11<00:01,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 43 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  94%|█████████▍| 45/48 [00:12<00:00,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 44 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files:  96%|█████████▌| 46/48 [00:12<00:00,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 45 shape: (58294, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading batch files: 100%|██████████| 48/48 [00:12<00:00,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 46 shape: (58294, 100)\n",
      "Batch 47 shape: (58294, 56)\n",
      "Concatenating all data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final data shape: (58294, 4756)\n",
      "Writing final output...\n",
      "Processing complete. Output written to PPMI_PD_Counts_24072024.tsv\n",
      "Loading data from PPMI_PD_Counts_24072024.tsv\n",
      "Original dataset shape: (58294, 4756)\n",
      "Filtered dataset shape: (150, 1768)\n",
      "Filtered data saved to PPMI_PD_Counts_Feature1.tsv\n",
      "Processing complete. Filtered output written to PPMI_PD_Counts_Feature1.tsv\n",
      "\n",
      "Filtered dataset shape: (150, 1768)\n",
      "Number of genes: 150\n",
      "Number of samples: 1768\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 1 -Load configuration\n",
    "    config = load_config('config.yaml')\n",
    "\n",
    "    # 2 - Process Salmon quantification files\n",
    "    main(config['input_folder'], \n",
    "         config['output_file'], \n",
    "         config['columns_to_extract'], \n",
    "         config['batch_size'],\n",
    "         config['sample_id_pattern'])\n",
    "\n",
    "    # 3 - Load filters\n",
    "    Patients = pd.read_csv(config['patients_file'], sep='\\t')\n",
    "    Patients_filter = Patients[config['patients_id_column']]\n",
    "\n",
    "    Meta_Analysis = pd.read_csv(config['meta_analysis_file'])\n",
    "    Feat_1 = Meta_Analysis.sort_values(config['feature_importance_column'], ascending=False)['Gene'][:config['top_genes_count']].values\n",
    "\n",
    "    # 4 - Apply filters\n",
    "    filter_dataset(input_file=config['output_file'], \n",
    "                   output_file=config['filtered_output_file'], \n",
    "                   row_condition=Feat_1, \n",
    "                   column_condition=Patients_filter)\n",
    "\n",
    "    print(f\"Processing complete. Filtered output written to {config['filtered_output_file']}\")\n",
    "\n",
    "    # Optional: Display summary of filtered data\n",
    "    filtered_data = pd.read_csv(config['filtered_output_file'], sep='\\t', index_col='Name')\n",
    "    print(f\"\\nFiltered dataset shape: {filtered_data.shape}\")\n",
    "    print(f\"Number of genes: {filtered_data.shape[0]}\")\n",
    "    print(f\"Number of samples: {filtered_data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af0321a-765c-4f32-b3d3-446a75416ca4",
   "metadata": {},
   "source": [
    "# 4 - Conclusion\n",
    "\n",
    "This notebook demonstrates a efficient method for consolidating and processing Salmon quantification data using Python. The workflow is designed to handle RNA-seq datasets.\n",
    "\n",
    "Key advantages of this approach include:\n",
    "\n",
    "- Parallel processing for improved performance\n",
    "- Batch processing to manage memory usage with large datasets\n",
    "- Flexibility in data extraction (TPM, NumReads, or other Salmon output columns)\n",
    "- Custom filtering capabilities for downstream analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
